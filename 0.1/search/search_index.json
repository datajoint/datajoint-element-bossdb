{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Element ZStack", "text": "<p>DataJoint Element for z-stack (volumetric) imaging, features cell segmentation with  cellpose, data upload to  BossDB, and data visualization with  Neuroglancer. DataJoint  Elements collectively standardize and automate data collection and analysis for  neuroscience experiments. Each Element is a modular pipeline for data storage and  processing with corresponding database tables that can be combined with other Elements  to assemble a fully functional pipeline.</p>"}, {"location": "#experiment-flowchart", "title": "Experiment Flowchart", "text": ""}, {"location": "#data-pipeline-diagram", "title": "Data Pipeline Diagram", "text": ""}, {"location": "#getting-started", "title": "Getting Started", "text": "<ul> <li>Install from PyPI<pre><code>pip install element-zstack\n</code></pre> </li> </ul> <ul> <li>Data Pipeline - Pipeline and table descriptions</li> </ul> <ul> <li>Tutorials - Start building your data pipeline</li> </ul> <ul> <li>Code Repository</li> </ul>"}, {"location": "#support", "title": "Support", "text": "<ul> <li>If you need help getting started or run into any errors, please contact our team by email at support@datajoint.com.</li> </ul>"}, {"location": "changelog/", "title": "Changelog", "text": "<p>Observes Semantic Versioning standard and Keep a Changelog convention.</p>"}, {"location": "changelog/#010-2023-05-10", "title": "0.1.0 - 2023-05-10", "text": "<ul> <li>Add - <code>volume</code> schema</li> <li>Add - <code>bossdb</code> schema</li> <li>Add - Upload utilty to create resources and upload data to BossDB</li> </ul>"}, {"location": "citation/", "title": "Citation", "text": "<p>If your work uses this Element, please cite the following manuscript and Research Resource Identifier (RRID).</p> <ul> <li>Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D, Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for Neurophysiology. bioRxiv. 2021 Jan 1. doi: https://doi.org/10.1101/2021.03.30.437358</li> </ul> <ul> <li>DataJoint Element ZStack - RRID:SCR_021894 - Version 0.1.0</li> </ul>"}, {"location": "concepts/", "title": "Concepts", "text": "<p>Element ZStack consists of a pipeline to perform cell segmentation of volumetric  microscopic images with  cellpose, data upload to  the Brain Observatory Storage Service and Database (BossDB),  and data visualization with  Neuroglancer.</p>"}, {"location": "partnerships/", "title": "Key Partnerships", "text": "<p>Labs have developed pipelines for volumetric data. The DataJoint team interviewed and collaborated with these teams to understand their experiment workflow, associated tools, and interfaces. These teams include:</p> <ul> <li>Andreas Tolias Lab, Baylor College of Medicine</li> <li>Applied Physics Laboratory, Johns Hopkins University</li> </ul>"}, {"location": "pipeline/", "title": "Data Pipeline", "text": "<p>Element ZStack is composed of two main schemas, <code>volume</code> and <code>bossdb</code>. Data  export to BossDB is handled with the <code>bossdb</code> schema and upload utilities.</p> <ul> <li><code>volume</code> module - performs segmentation of volumetric microscopic images with  <code>cellpose</code>.</li> </ul> <ul> <li><code>bossdb</code> module - uploads data to BossDB, creates a Neuroglancer visualization, and  stores the relevant URLs.</li> </ul> <p>Each node in the following diagram represents the analysis code in the pipeline and the corresponding table in the database.  Within the workflow, Element ZStack connects to upstream Elements including Lab, Animal, Session, and Calcium Imaging. For  more detailed documentation on each table, see the API docs for the respective schemas.</p> <p></p>"}, {"location": "pipeline/#reference-schema-api-docs", "title": "<code>reference</code> schema (API docs)", "text": "Table Description Device Lab equipment metadata"}, {"location": "pipeline/#subject-schema-api-docs", "title": "<code>subject</code> schema (API docs)", "text": "<ul> <li>Although not required, most choose to connect the <code>Session</code> table to a <code>Subject</code> table.</li> </ul> Table Description Subject Basic information of the research subject"}, {"location": "pipeline/#session-schema-api-docs", "title": "<code>session</code> schema (API docs)", "text": "Table Description Session Unique experimental session identifier"}, {"location": "pipeline/#scan-schema-api-docs", "title": "<code>scan</code> schema (API docs)", "text": "Table Description Scan A set of imaging scans performed in a single session"}, {"location": "pipeline/#volume-schema-api-docs", "title": "<code>volume</code> schema (API docs)", "text": "Table Description Volume Details about the volumetric microscopic images VoxelSize Voxel size information about a volume in millimeters SegmentationParamSet Parameters required for segmentation of the volumetric scans SegmentationTask Task defined by a combination of Volume and SegmentationParamSet Segmentation The core table that executes a SegmentationTask Segmentation.Mask Details of the masks identified from the segmentation"}, {"location": "pipeline/#bossdb-schema-api-docs", "title": "<code>bossdb</code> schema (API docs)", "text": "Table Description VolumeUploadTask Names of the collection, experiment, and channel where data will be uploaded to BossDB VolumeUpload Uploads image and segmentation data to BossDB VolumeUpload.WebAddress Stores the BossDB and Neuroglancer URLs for each upload"}, {"location": "roadmap/", "title": "Roadmap", "text": "<p>Through our interviews and direct collaboration on key projects, we identified the common motifs to create Element ZStack.  Features of this Element include:</p> <ul> <li> Ingest the metadata of a volumetric image.</li> <li> Perform cell segmentation of a volumetric image using cellpose.</li> <li> Upload the volumetric image and cell segmentations to BossDB and store the BossDB URL.</li> <li> Generate a link to visualize the image and segmentations with Neuroglancer and store the URL.</li> </ul> <p>Further development of this Element is community driven.  Upon user requests and based  on guidance from the Scientific Steering Group we will add further features to this  Element.</p>"}, {"location": "api/element_zstack/bossdb/", "title": "bossdb.py", "text": ""}, {"location": "api/element_zstack/bossdb/#element_zstack.bossdb.BossDBUpload", "title": "<code>BossDBUpload</code>", "text": "<p>Upload data to bossdb from a DataJoint pipeline.</p> Source code in <code>element_zstack/export/bossdb_interface.py</code> <pre><code>class BossDBUpload:\n\"\"\"Upload data to bossdb from a DataJoint pipeline.\"\"\"\n\n    def __init__(\n        self,\n        url: str,\n        volume_data: np.ndarray,  # Numpy array of the volumetric data\n        data_description: str,\n        voxel_size: Tuple[int, int, int],  # voxel size in ZYX order\n        voxel_units: str,  # The size units of a voxel\n        upload_increment: Optional[int] = 16,  # How many z slices to upload at once\n        retry_max: Optional[int] = 3,  # Number of retries to upload a single increment\n        overwrite: Optional[bool] = False,  # Overwrite existing data\n    ):\n\"\"\"Required information for data upload to bossdb.\n\n        Args:\n            url (str): Bossdb URL where data will be uploaded.\n            volume_data (np.ndarray): Numpy array of the volumetric data\n            data_description (str): either `image` or `annotation`.\n            voxel_size (Tuple[int, int, int]): Voxel size of the image in z,y,x\n            format.\n            voxel_units (str): Voxel units as string.\n            upload_increment (int, optional): Number of z slices to upload at\n            once.\n            retry_max (int, optional): Number of retries to upload a single\n            increment.\n            overwrite (bool, optional): Overwrite existing data.\n        \"\"\"\n\n        self._url = url\n        self.url_bits = _parse_bossdb_uri(url)\n        self._volume_data = volume_data\n        self._voxel_size = tuple(float(i) for i in voxel_size)\n        self._voxel_units = voxel_units\n        self._data_description = data_description\n        self._upload_increment = upload_increment\n        self._retry_max = retry_max\n        self._overwrite = overwrite\n        self.description = \"Uploaded via DataJoint\"\n        self._resources = dict()\n        self._shape_zyx = tuple(int(i) for i in self._volume_data.shape)\n\n        try:\n            type(array(self._url))\n        except HTTPError:\n            self.url_exists = False\n        else:\n            self.url_exists = True\n        if not overwrite and self.url_exists:\n            logger.warning(\n                f\"Dataset already exists at {self._url}\\n\"\n                + \" To overwrite, set `overwrite` to True\"\n            )\n            return\n\n        if not self.url_exists:\n            self.try_create_new()\n\n    def upload(self):\n\"\"\"Upload data to bossdb.\"\"\"\n        z_max = self._shape_zyx[0]\n        boss_dataset = array(\n            self._url,\n            extents=self._shape_zyx,\n            dtype=str(self._volume_data.dtype),\n            voxel_size=self._voxel_size,\n            voxel_unit=self._voxel_units,\n        )\n        for i in tqdm(range(0, z_max, self._upload_increment)):\n            if i + self._upload_increment &gt; self._shape_zyx[0]:\n                # We're at the end of the stack, so upload the remaining images.\n                images = self._volume_data[i : self._shape_zyx[0], :, :]\n                retry_count = 0\n                while True:\n                    try:\n                        boss_dataset[\n                            i : i + images.shape[0],\n                            0 : images.shape[1],\n                            0 : images.shape[2],\n                        ] = images\n                        break\n                    except Exception as e:\n                        print(f\"Error uploading chunk {i}-{i + images.shape[0]}: {e}\")\n                        retry_count += 1\n                        if retry_count &gt; self._retry_max:\n                            raise e\n                        print(\"Retrying...\")\n                        continue\n            else:\n                images = self._volume_data[i : i + self._upload_increment]\n                retry_count = 0\n                while True:\n                    try:\n                        boss_dataset[\n                            i : i + images.shape[0],\n                            0 : images.shape[1],\n                            0 : images.shape[2],\n                        ] = images\n                        break\n                    except Exception as e:\n                        print(f\"Error uploading chunk {i}-{i + images.shape[0]}: {e}\")\n                        retry_count += 1\n                        if retry_count &gt; self._retry_max:\n                            raise e\n                        print(\"Retrying...\")\n                        continue\n\n    @property\n    def resources(self):\n\"\"\"Create the resources objects required for uploading data to bossdb.\"\"\"\n        # Default resources for creating channels\n        coord_name = f\"CF_{self.url_bits.collection}_{self.url_bits.experiment}\"\n        if not self._resources:\n            self._resources = dict(\n                collection=CollectionResource(\n                    name=self.url_bits.collection, description=self.description\n                ),\n                coord_frame=CoordinateFrameResource(\n                    name=coord_name,\n                    description=self.description,\n                    x_start=0,\n                    x_stop=self._shape_zyx[2],\n                    y_start=0,\n                    y_stop=self._shape_zyx[1],\n                    z_start=0,\n                    z_stop=self._shape_zyx[0],\n                    x_voxel_size=self._voxel_size[2],\n                    y_voxel_size=self._voxel_size[1],\n                    z_voxel_size=self._voxel_size[0],\n                ),\n                experiment=ExperimentResource(\n                    name=self.url_bits.experiment,\n                    collection_name=self.url_bits.collection,\n                    coord_frame=coord_name,\n                    description=self.description,\n                ),\n                channel_resource=ChannelResource(\n                    name=self.url_bits.channel,\n                    collection_name=self.url_bits.collection,\n                    experiment_name=self.url_bits.experiment,\n                    type=self._data_description,\n                    description=self.description,\n                    datatype=str(self._volume_data.dtype),\n                ),\n                channel=ChannelResource(\n                    name=self.url_bits.channel,\n                    collection_name=self.url_bits.collection,\n                    experiment_name=self.url_bits.experiment,\n                    type=self._data_description,\n                    description=self.description,\n                    datatype=str(self._volume_data.dtype),\n                    sources=[],\n                ),\n            )\n        return self._resources\n\n    def try_create_new(self):\n\"\"\"Create new resources on bossdb.\"\"\"\n        remote = BossRemote()\n\n        # Make collection\n        _ = self._get_or_create(remote=remote, obj=self.resources[\"collection\"])\n\n        # Make coord frame\n        true_coord_frame = self._get_or_create(\n            remote=remote, obj=self.resources[\"coord_frame\"]\n        )\n\n        # Set Experiment based on coord frame\n        experiment = self.resources[\"experiment\"]\n        experiment.coord_frame = true_coord_frame.name\n        _ = self._get_or_create(remote=remote, obj=experiment)\n\n        # Set channel based on resource\n        channel_resource = self._get_or_create(\n            remote=remote, obj=self.resources[\"channel_resource\"]\n        )\n        channel = self.resources[\"channel\"]\n        channel.sources = [channel_resource.name]\n        _ = self._get_or_create(remote=remote, obj=channel)\n\n    def _get_or_create(self, remote, obj):\n\"\"\"Check if a resource exists on bossdb.\"\"\"\n        try:\n            result = remote.get_project(obj)\n        except HTTPError:\n            logger.info(f\"Creating {obj.name}\")\n            result = remote.create_project(obj)\n        return result\n</code></pre>"}, {"location": "api/element_zstack/bossdb/#element_zstack.export.bossdb_interface.BossDBUpload.__init__", "title": "<code>__init__(url, volume_data, data_description, voxel_size, voxel_units, upload_increment=16, retry_max=3, overwrite=False)</code>", "text": "<p>Required information for data upload to bossdb.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Bossdb URL where data will be uploaded.</p> required <code>volume_data</code> <code>np.ndarray</code> <p>Numpy array of the volumetric data</p> required <code>data_description</code> <code>str</code> <p>either <code>image</code> or <code>annotation</code>.</p> required <code>voxel_size</code> <code>Tuple[int, int, int]</code> <p>Voxel size of the image in z,y,x</p> required <code>voxel_units</code> <code>str</code> <p>Voxel units as string.</p> required <code>upload_increment</code> <code>int</code> <p>Number of z slices to upload at</p> <code>16</code> <code>retry_max</code> <code>int</code> <p>Number of retries to upload a single</p> <code>3</code> <code>overwrite</code> <code>bool</code> <p>Overwrite existing data.</p> <code>False</code> Source code in <code>element_zstack/export/bossdb_interface.py</code> <pre><code>def __init__(\n    self,\n    url: str,\n    volume_data: np.ndarray,  # Numpy array of the volumetric data\n    data_description: str,\n    voxel_size: Tuple[int, int, int],  # voxel size in ZYX order\n    voxel_units: str,  # The size units of a voxel\n    upload_increment: Optional[int] = 16,  # How many z slices to upload at once\n    retry_max: Optional[int] = 3,  # Number of retries to upload a single increment\n    overwrite: Optional[bool] = False,  # Overwrite existing data\n):\n\"\"\"Required information for data upload to bossdb.\n\n    Args:\n        url (str): Bossdb URL where data will be uploaded.\n        volume_data (np.ndarray): Numpy array of the volumetric data\n        data_description (str): either `image` or `annotation`.\n        voxel_size (Tuple[int, int, int]): Voxel size of the image in z,y,x\n        format.\n        voxel_units (str): Voxel units as string.\n        upload_increment (int, optional): Number of z slices to upload at\n        once.\n        retry_max (int, optional): Number of retries to upload a single\n        increment.\n        overwrite (bool, optional): Overwrite existing data.\n    \"\"\"\n\n    self._url = url\n    self.url_bits = _parse_bossdb_uri(url)\n    self._volume_data = volume_data\n    self._voxel_size = tuple(float(i) for i in voxel_size)\n    self._voxel_units = voxel_units\n    self._data_description = data_description\n    self._upload_increment = upload_increment\n    self._retry_max = retry_max\n    self._overwrite = overwrite\n    self.description = \"Uploaded via DataJoint\"\n    self._resources = dict()\n    self._shape_zyx = tuple(int(i) for i in self._volume_data.shape)\n\n    try:\n        type(array(self._url))\n    except HTTPError:\n        self.url_exists = False\n    else:\n        self.url_exists = True\n    if not overwrite and self.url_exists:\n        logger.warning(\n            f\"Dataset already exists at {self._url}\\n\"\n            + \" To overwrite, set `overwrite` to True\"\n        )\n        return\n\n    if not self.url_exists:\n        self.try_create_new()\n</code></pre>"}, {"location": "api/element_zstack/bossdb/#element_zstack.export.bossdb_interface.BossDBUpload.upload", "title": "<code>upload()</code>", "text": "<p>Upload data to bossdb.</p> Source code in <code>element_zstack/export/bossdb_interface.py</code> <pre><code>def upload(self):\n\"\"\"Upload data to bossdb.\"\"\"\n    z_max = self._shape_zyx[0]\n    boss_dataset = array(\n        self._url,\n        extents=self._shape_zyx,\n        dtype=str(self._volume_data.dtype),\n        voxel_size=self._voxel_size,\n        voxel_unit=self._voxel_units,\n    )\n    for i in tqdm(range(0, z_max, self._upload_increment)):\n        if i + self._upload_increment &gt; self._shape_zyx[0]:\n            # We're at the end of the stack, so upload the remaining images.\n            images = self._volume_data[i : self._shape_zyx[0], :, :]\n            retry_count = 0\n            while True:\n                try:\n                    boss_dataset[\n                        i : i + images.shape[0],\n                        0 : images.shape[1],\n                        0 : images.shape[2],\n                    ] = images\n                    break\n                except Exception as e:\n                    print(f\"Error uploading chunk {i}-{i + images.shape[0]}: {e}\")\n                    retry_count += 1\n                    if retry_count &gt; self._retry_max:\n                        raise e\n                    print(\"Retrying...\")\n                    continue\n        else:\n            images = self._volume_data[i : i + self._upload_increment]\n            retry_count = 0\n            while True:\n                try:\n                    boss_dataset[\n                        i : i + images.shape[0],\n                        0 : images.shape[1],\n                        0 : images.shape[2],\n                    ] = images\n                    break\n                except Exception as e:\n                    print(f\"Error uploading chunk {i}-{i + images.shape[0]}: {e}\")\n                    retry_count += 1\n                    if retry_count &gt; self._retry_max:\n                        raise e\n                    print(\"Retrying...\")\n                    continue\n</code></pre>"}, {"location": "api/element_zstack/bossdb/#element_zstack.export.bossdb_interface.BossDBUpload.resources", "title": "<code>resources</code>  <code>property</code>", "text": "<p>Create the resources objects required for uploading data to bossdb.</p>"}, {"location": "api/element_zstack/bossdb/#element_zstack.export.bossdb_interface.BossDBUpload.try_create_new", "title": "<code>try_create_new()</code>", "text": "<p>Create new resources on bossdb.</p> Source code in <code>element_zstack/export/bossdb_interface.py</code> <pre><code>def try_create_new(self):\n\"\"\"Create new resources on bossdb.\"\"\"\n    remote = BossRemote()\n\n    # Make collection\n    _ = self._get_or_create(remote=remote, obj=self.resources[\"collection\"])\n\n    # Make coord frame\n    true_coord_frame = self._get_or_create(\n        remote=remote, obj=self.resources[\"coord_frame\"]\n    )\n\n    # Set Experiment based on coord frame\n    experiment = self.resources[\"experiment\"]\n    experiment.coord_frame = true_coord_frame.name\n    _ = self._get_or_create(remote=remote, obj=experiment)\n\n    # Set channel based on resource\n    channel_resource = self._get_or_create(\n        remote=remote, obj=self.resources[\"channel_resource\"]\n    )\n    channel = self.resources[\"channel\"]\n    channel.sources = [channel_resource.name]\n    _ = self._get_or_create(remote=remote, obj=channel)\n</code></pre>"}, {"location": "api/element_zstack/bossdb/#element_zstack.bossdb.activate", "title": "<code>activate(schema_name, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activate this schema</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>schema name on the database server to activate the <code>bossdb</code> schema</p> required <code>create_schema</code> <code>bool</code> <p>when True (default), create schema in the database if it                 does not yet exist.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>when True (default), create schema tables in the database                  if they do not yet exist.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A string containing the module name or module containing the required dependencies to activate the schema.</p> <code>None</code> <p>Dependencies:</p> Tables <p>volume.VolumeSegmentation: A parent table to VolumeUploadTask volume.VoxelSize: A dependency of VolumeUpload</p> Functions <p>get_volume_root_data_dir: Returns absolute path for root data director(y/ies) with all volumetric data, as a list of string(s).</p> Source code in <code>element_zstack/bossdb.py</code> <pre><code>def activate(\n    schema_name: str,\n    *,\n    create_schema: bool = True,\n    create_tables: bool = True,\n    linking_module: str = None,\n):\n\"\"\"Activate this schema\n\n    Args:\n        schema_name (str): schema name on the database server to activate the `bossdb` schema\n        create_schema (bool): when True (default), create schema in the database if it\n                            does not yet exist.\n        create_tables (bool): when True (default), create schema tables in the database\n                             if they do not yet exist.\n        linking_module (str): A string containing the module name or module containing\n            the required dependencies to activate the schema.\n\n    Dependencies:\n    Tables:\n        volume.VolumeSegmentation: A parent table to VolumeUploadTask\n        volume.VoxelSize: A dependency of VolumeUpload\n    Functions:\n        get_volume_root_data_dir: Returns absolute path for root data\n        director(y/ies) with all volumetric data, as a list of string(s).\n    \"\"\"\n\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'linking_module' must be a module's name or a module\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    schema.activate(\n        schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n</code></pre>"}, {"location": "api/element_zstack/bossdb/#element_zstack.bossdb.get_volume_root_data_dir", "title": "<code>get_volume_root_data_dir()</code>", "text": "<p>Fetches absolute data path to volume data directories.</p> <p>The absolute path here is used as a reference for all downstream relative paths used in DataJoint.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list of the absolute path(s) to volume data directories.</p> Source code in <code>element_zstack/bossdb.py</code> <pre><code>def get_volume_root_data_dir() -&gt; list:\n\"\"\"Fetches absolute data path to volume data directories.\n\n    The absolute path here is used as a reference for all downstream relative paths used in DataJoint.\n\n    Returns:\n        A list of the absolute path(s) to volume data directories.\n    \"\"\"\n    root_directories = _linking_module.get_volume_root_data_dir()\n    if isinstance(root_directories, (str, Path)):\n        root_directories = [root_directories]\n\n    return root_directories\n</code></pre>"}, {"location": "api/element_zstack/bossdb/#element_zstack.bossdb.VolumeUploadTask", "title": "<code>VolumeUploadTask</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Define the image and segmentation data to upload to BossDB.</p> <p>Attributes:</p> Name Type Description <code>volume.Segmentation</code> <code>foreign key</code> <p>Primary key from <code>volume.Segmentation</code>.</p> <code>collection_name</code> <code>str</code> <p>Name of the collection on BossDB.</p> <code>experiment_name</code> <code>str</code> <p>Name of the experiment on BossDB.</p> <code>channel_name</code> <code>str</code> <p>Name of the channel on BossDB.</p> Source code in <code>element_zstack/bossdb.py</code> <pre><code>@schema\nclass VolumeUploadTask(dj.Manual):\n\"\"\"Define the image and segmentation data to upload to BossDB.\n\n    Attributes:\n        volume.Segmentation (foreign key): Primary key from `volume.Segmentation`.\n        collection_name (str): Name of the collection on BossDB.\n        experiment_name (str): Name of the experiment on BossDB.\n        channel_name (str): Name of the channel on BossDB.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; volume.Segmentation\n    ---\n    collection_name: varchar(64)\n    experiment_name: varchar(64)\n    channel_name: varchar(64)\n    \"\"\"\n</code></pre>"}, {"location": "api/element_zstack/bossdb/#element_zstack.bossdb.VolumeUpload", "title": "<code>VolumeUpload</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Upload image and segmentation data to BossDB, and store the BossDB and Neuroglancer URLs.</p> <p>Attributes:</p> Name Type Description <code>VolumeUploadTask</code> <code>foreign key</code> <p>Primary key from <code>VolumeUploadTask</code>.</p> <code>volume.VoxelSize</code> <code>foreign key</code> <p>Primary key from <code>volume.VoxelSize</code>.</p> Source code in <code>element_zstack/bossdb.py</code> <pre><code>@schema\nclass VolumeUpload(dj.Computed):\n\"\"\"Upload image and segmentation data to BossDB, and store the BossDB and Neuroglancer URLs.\n\n    Attributes:\n        VolumeUploadTask (foreign key): Primary key from `VolumeUploadTask`.\n        volume.VoxelSize (foreign key): Primary key from `volume.VoxelSize`.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; VolumeUploadTask\n    ---\n    -&gt; volume.VoxelSize\n    \"\"\"\n\n    class WebAddress(dj.Part):\n\"\"\"\n        Attributes:\n            VolumeUpload (foreign key): Primary key from `VolumeUpload`.\n            upload_type (enum): 'image' (volumetric image), 'annotation' (segmentation data), or 'image+annotation' (segmentation overlayed on image).\n            web_address_type (enum): 'bossdb' or 'neuroglancer'.\n            web_address (str): URL for the data or visualization website.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        upload_type='image': enum('image', 'annotation', 'image+annotation')\n        web_address_type='bossdb': enum('bossdb', 'neuroglancer')\n        ---\n        web_address: varchar(2048)\n        \"\"\"\n\n    def get_neuroglancer_url(self, upload_type, collection, experiment, channel):\n        base_url = f\"boss://https://api.bossdb.io/{collection}/{experiment}/{channel}\"\n        if upload_type == \"image\":\n            return \"https://neuroglancer.bossdb.io/#!\" + str(\n                {\n                    \"layers\": {\n                        \"type\": \"image\",\n                        \"source\": base_url,\n                        \"tab\": \"source\",\n                        \"name\": f\"{experiment}\",\n                    }\n                }\n            )\n        elif upload_type == \"annotation\":\n            return \"https://neuroglancer.bossdb.io/#!\" + str(\n                {\n                    \"layers\": {\n                        \"type\": \"segmentation\",\n                        \"source\": base_url + \"-seg\",\n                        \"tab\": \"annotations\",\n                        \"name\": f\"{channel}-seg\",\n                    }\n                }\n            )\n        elif upload_type == \"image+annotation\":\n            return \"https://neuroglancer.bossdb.io/#!\" + str(\n                {\n                    \"layers\": [\n                        {\n                            \"type\": \"image\",\n                            \"source\": base_url,\n                            \"tab\": \"source\",\n                            \"name\": f\"{experiment}\",\n                        },\n                        {\n                            \"type\": \"segmentation\",\n                            \"source\": base_url + \"-seg\",\n                            \"tab\": \"annotations\",\n                            \"name\": f\"{channel}-seg\",\n                        },\n                    ]\n                }\n            )\n\n    def make(self, key):\n\"\"\"Upload data to bossdb.\"\"\"\n\n        collection, experiment, channel = (VolumeUploadTask &amp; key).fetch1(\n            \"collection_name\", \"experiment_name\", \"channel_name\"\n        )\n\n        voxel_width, voxel_height, voxel_depth = (volume.VoxelSize &amp; key).fetch1(\n            \"width\", \"height\", \"depth\"\n        )\n\n        description = [\"image\", \"annotation\", \"image+annotation\"]\n        full_data = []\n        boss_url = []\n        neuroglancer_url = []\n\n        volume_relative_path = (volume.Volume &amp; key).fetch1(\"volume_file_path\")\n        volume_file_path = find_full_path(\n            get_volume_root_data_dir(), volume_relative_path\n        ).as_posix()\n        volume_data = TiffFile(volume_file_path).asarray()\n\n        full_data.append(volume_data)\n        boss_url.append(f\"bossdb://{collection}/{experiment}/{channel}\")\n\n        z_size, y_size, x_size = (volume.Volume &amp; key).fetch1(\n            \"px_depth\", \"px_height\", \"px_width\"\n        )\n        segmentation_data = np.zeros((z_size, y_size, x_size))\n\n        mask_ids, x_mask_pix, y_mask_pix, z_mask_pix = (\n            volume.Segmentation.Mask &amp; key\n        ).fetch(\"mask\", \"mask_xpix\", \"mask_ypix\", \"mask_zpix\")\n\n        for idx, mask in enumerate(mask_ids):\n            segmentation_data[\n                np.s_[z_mask_pix[idx], y_mask_pix[idx], x_mask_pix[idx]]\n            ] = mask\n        full_data.append(segmentation_data.astype(\"uint64\"))\n\n        boss_url.append(f\"bossdb://{collection}/{experiment}/{channel}-seg\")\n\n        for url, data, desc in zip(boss_url, full_data, description[:2]):\n            BossDBUpload(\n                url=url,\n                volume_data=data,\n                data_description=desc,\n                voxel_size=(voxel_depth, voxel_height, voxel_width),\n                voxel_units=\"millimeters\",\n            ).upload()\n\n        self.insert1(key)\n        self.WebAddress.insert(\n            [\n                dict(\n                    key,\n                    upload_type=desc,\n                    web_address_type=\"bossdb\",\n                    web_address=db_url,\n                )\n                for desc, db_url in list(zip(description[:2], boss_url))\n            ]\n        )\n\n        for desc in description:\n            neuroglancer_url.append(\n                self.get_neuroglancer_url(desc, collection, experiment, channel)\n            )\n        self.WebAddress.insert(\n            [\n                dict(\n                    key,\n                    upload_type=desc,\n                    web_address_type=\"neuroglancer\",\n                    web_address=url,\n                )\n                for desc, url in list(zip(description, neuroglancer_url))\n            ]\n        )\n</code></pre>"}, {"location": "api/element_zstack/bossdb/#element_zstack.bossdb.VolumeUpload.WebAddress", "title": "<code>WebAddress</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Attributes:</p> Name Type Description <code>VolumeUpload</code> <code>foreign key</code> <p>Primary key from <code>VolumeUpload</code>.</p> <code>upload_type</code> <code>enum</code> <p>'image' (volumetric image), 'annotation' (segmentation data), or 'image+annotation' (segmentation overlayed on image).</p> <code>web_address_type</code> <code>enum</code> <p>'bossdb' or 'neuroglancer'.</p> <code>web_address</code> <code>str</code> <p>URL for the data or visualization website.</p> Source code in <code>element_zstack/bossdb.py</code> <pre><code>class WebAddress(dj.Part):\n\"\"\"\n    Attributes:\n        VolumeUpload (foreign key): Primary key from `VolumeUpload`.\n        upload_type (enum): 'image' (volumetric image), 'annotation' (segmentation data), or 'image+annotation' (segmentation overlayed on image).\n        web_address_type (enum): 'bossdb' or 'neuroglancer'.\n        web_address (str): URL for the data or visualization website.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    upload_type='image': enum('image', 'annotation', 'image+annotation')\n    web_address_type='bossdb': enum('bossdb', 'neuroglancer')\n    ---\n    web_address: varchar(2048)\n    \"\"\"\n</code></pre>"}, {"location": "api/element_zstack/bossdb/#element_zstack.bossdb.VolumeUpload.make", "title": "<code>make(key)</code>", "text": "<p>Upload data to bossdb.</p> Source code in <code>element_zstack/bossdb.py</code> <pre><code>def make(self, key):\n\"\"\"Upload data to bossdb.\"\"\"\n\n    collection, experiment, channel = (VolumeUploadTask &amp; key).fetch1(\n        \"collection_name\", \"experiment_name\", \"channel_name\"\n    )\n\n    voxel_width, voxel_height, voxel_depth = (volume.VoxelSize &amp; key).fetch1(\n        \"width\", \"height\", \"depth\"\n    )\n\n    description = [\"image\", \"annotation\", \"image+annotation\"]\n    full_data = []\n    boss_url = []\n    neuroglancer_url = []\n\n    volume_relative_path = (volume.Volume &amp; key).fetch1(\"volume_file_path\")\n    volume_file_path = find_full_path(\n        get_volume_root_data_dir(), volume_relative_path\n    ).as_posix()\n    volume_data = TiffFile(volume_file_path).asarray()\n\n    full_data.append(volume_data)\n    boss_url.append(f\"bossdb://{collection}/{experiment}/{channel}\")\n\n    z_size, y_size, x_size = (volume.Volume &amp; key).fetch1(\n        \"px_depth\", \"px_height\", \"px_width\"\n    )\n    segmentation_data = np.zeros((z_size, y_size, x_size))\n\n    mask_ids, x_mask_pix, y_mask_pix, z_mask_pix = (\n        volume.Segmentation.Mask &amp; key\n    ).fetch(\"mask\", \"mask_xpix\", \"mask_ypix\", \"mask_zpix\")\n\n    for idx, mask in enumerate(mask_ids):\n        segmentation_data[\n            np.s_[z_mask_pix[idx], y_mask_pix[idx], x_mask_pix[idx]]\n        ] = mask\n    full_data.append(segmentation_data.astype(\"uint64\"))\n\n    boss_url.append(f\"bossdb://{collection}/{experiment}/{channel}-seg\")\n\n    for url, data, desc in zip(boss_url, full_data, description[:2]):\n        BossDBUpload(\n            url=url,\n            volume_data=data,\n            data_description=desc,\n            voxel_size=(voxel_depth, voxel_height, voxel_width),\n            voxel_units=\"millimeters\",\n        ).upload()\n\n    self.insert1(key)\n    self.WebAddress.insert(\n        [\n            dict(\n                key,\n                upload_type=desc,\n                web_address_type=\"bossdb\",\n                web_address=db_url,\n            )\n            for desc, db_url in list(zip(description[:2], boss_url))\n        ]\n    )\n\n    for desc in description:\n        neuroglancer_url.append(\n            self.get_neuroglancer_url(desc, collection, experiment, channel)\n        )\n    self.WebAddress.insert(\n        [\n            dict(\n                key,\n                upload_type=desc,\n                web_address_type=\"neuroglancer\",\n                web_address=url,\n            )\n            for desc, url in list(zip(description, neuroglancer_url))\n        ]\n    )\n</code></pre>"}, {"location": "api/element_zstack/version/", "title": "version.py", "text": "<p>Package metadata.</p>"}, {"location": "api/element_zstack/volume/", "title": "volume.py", "text": ""}, {"location": "api/element_zstack/volume/#element_zstack.volume.activate", "title": "<code>activate(schema_name, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activate this schema</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>schema name on the database server to activate the <code>zstack</code> element</p> required <code>create_schema</code> <code>bool</code> <p>when True (default), create schema in the database</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>when True (default), create schema tables in the database if they do not yet exist.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A string containing the module name or module</p> <code>None</code> Tables <p>Scan: A parent table to Volume</p> Functions <p>get_volume_root_data_dir: Returns absolute path for root data director(y/ies) with all volumetric data, as a list of string(s). get_volume_tif_file: When given a scan key (dict), returns the full path to the TIF file of the volumetric data associated with a given scan.</p> Source code in <code>element_zstack/volume.py</code> <pre><code>def activate(\n    schema_name: str,\n    *,\n    create_schema: bool = True,\n    create_tables: bool = True,\n    linking_module: str = None,\n):\n\"\"\"Activate this schema\n\n    Args:\n        schema_name (str): schema name on the database server to activate the `zstack` element\n        create_schema (bool): when True (default), create schema in the database\n        if it does not yet exist.\n        create_tables (bool): when True (default), create schema tables in the database if they do not yet exist.\n        linking_module (str): A string containing the module name or module\n        containing the required dependencies to activate the schema.\n\n    Tables:\n        Scan: A parent table to Volume\n    Functions:\n        get_volume_root_data_dir: Returns absolute path for root data\n        director(y/ies) with all volumetric data, as a list of string(s).\n        get_volume_tif_file: When given a scan key (dict), returns the full path\n        to the TIF file of the volumetric data associated with a given scan.\n    \"\"\"\n\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'linking_module' must be a module's name or a module\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    schema.activate(\n        schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n</code></pre>"}, {"location": "api/element_zstack/volume/#element_zstack.volume.get_volume_root_data_dir", "title": "<code>get_volume_root_data_dir()</code>", "text": "<p>Fetches absolute data path to volume data directories.</p> <p>The absolute path here is used as a reference for all downstream relative paths used in DataJoint.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list of the absolute path(s) to volume data directories.</p> Source code in <code>element_zstack/volume.py</code> <pre><code>def get_volume_root_data_dir() -&gt; list:\n\"\"\"Fetches absolute data path to volume data directories.\n\n    The absolute path here is used as a reference for all downstream relative paths used in DataJoint.\n\n    Returns:\n        A list of the absolute path(s) to volume data directories.\n    \"\"\"\n    root_directories = _linking_module.get_volume_root_data_dir()\n    if isinstance(root_directories, (str, Path)):\n        root_directories = [root_directories]\n\n    return root_directories\n</code></pre>"}, {"location": "api/element_zstack/volume/#element_zstack.volume.get_volume_tif_file", "title": "<code>get_volume_tif_file(scan_key)</code>", "text": "<p>Retrieve the full path to the TIF file of the volumetric data associated with a given scan.</p> <p>Parameters:</p> Name Type Description Default <code>scan_key</code> <code>dict</code> <p>Primary key of a Scan entry.</p> required <p>Returns:</p> Type Description <code>str, Path</code> <p>Full path to the TIF file of the volumetric data (Path or str).</p> Source code in <code>element_zstack/volume.py</code> <pre><code>def get_volume_tif_file(scan_key: dict) -&gt; (str, Path):\n\"\"\"Retrieve the full path to the TIF file of the volumetric data associated with a given scan.\n    Args:\n        scan_key: Primary key of a Scan entry.\n    Returns:\n        Full path to the TIF file of the volumetric data (Path or str).\n    \"\"\"\n    return _linking_module.get_volume_tif_file(scan_key)\n</code></pre>"}, {"location": "api/element_zstack/volume/#element_zstack.volume.Volume", "title": "<code>Volume</code>", "text": "<p>         Bases: <code>dj.Imported</code></p> <p>Details about the volumetric microscopic imaging scans.</p> <p>Attributes:</p> Name Type Description <code>Scan</code> <code>foreign key</code> <p>Primary key from <code>imaging.Scan</code>.</p> <code>px_width</code> <code>int</code> <p>total number of voxels in the x dimension.</p> <code>px_height</code> <code>int</code> <p>total number of voxels in the y dimension.</p> <code>px_depth</code> <code>int</code> <p>total number of voxels in the z dimension.</p> <code>depth_mean_brightness</code> <code>longblob</code> <p>optional, mean brightness of each slice across</p> <code>volume_file_path</code> <code>str</code> <p>Relative path of the volumetric data with shape (z, y, x)</p> Source code in <code>element_zstack/volume.py</code> <pre><code>@schema\nclass Volume(dj.Imported):\n\"\"\"Details about the volumetric microscopic imaging scans.\n\n    Attributes:\n        Scan (foreign key): Primary key from `imaging.Scan`.\n        px_width (int): total number of voxels in the x dimension.\n        px_height (int): total number of voxels in the y dimension.\n        px_depth (int): total number of voxels in the z dimension.\n        depth_mean_brightness (longblob): optional, mean brightness of each slice across\n        the depth (z) dimension of the stack.\n        volume_file_path (str): Relative path of the volumetric data with shape (z, y, x)\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; Scan\n    ---\n    px_width: int # total number of voxels in x dimension\n    px_height: int # total number of voxels in y dimension\n    px_depth: int # total number of voxels in z dimension\n    depth_mean_brightness=null: longblob  # mean brightness of each slice across the depth (z) dimension of the stack\n    volume_file_path: varchar(255)  # Relative path of the volumetric data with shape (z, y, x)\n    \"\"\"\n\n    def make(self, key):\n\"\"\"Populate the Volume table with volumetric microscopic imaging data.\"\"\"\n        volume_file_path = get_volume_tif_file(key)\n        volume_data = TiffFile(volume_file_path).asarray()\n\n        root_dir = find_root_directory(get_volume_root_data_dir(), volume_file_path)\n        volume_relative_path = Path(volume_file_path).relative_to(root_dir).as_posix()\n\n        self.insert1(\n            dict(\n                **key,\n                volume_file_path=volume_relative_path,\n                px_width=volume_data.shape[2],\n                px_height=volume_data.shape[1],\n                px_depth=volume_data.shape[0],\n                depth_mean_brightness=volume_data.mean(axis=(1, 2)),\n            )\n        )\n</code></pre>"}, {"location": "api/element_zstack/volume/#element_zstack.volume.Volume.make", "title": "<code>make(key)</code>", "text": "<p>Populate the Volume table with volumetric microscopic imaging data.</p> Source code in <code>element_zstack/volume.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the Volume table with volumetric microscopic imaging data.\"\"\"\n    volume_file_path = get_volume_tif_file(key)\n    volume_data = TiffFile(volume_file_path).asarray()\n\n    root_dir = find_root_directory(get_volume_root_data_dir(), volume_file_path)\n    volume_relative_path = Path(volume_file_path).relative_to(root_dir).as_posix()\n\n    self.insert1(\n        dict(\n            **key,\n            volume_file_path=volume_relative_path,\n            px_width=volume_data.shape[2],\n            px_height=volume_data.shape[1],\n            px_depth=volume_data.shape[0],\n            depth_mean_brightness=volume_data.mean(axis=(1, 2)),\n        )\n    )\n</code></pre>"}, {"location": "api/element_zstack/volume/#element_zstack.volume.VoxelSize", "title": "<code>VoxelSize</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Voxel size information about a volume in millimeters.</p> <p>Attributes:</p> Name Type Description <code>Volume</code> <code>foreign key</code> <p>Primary key from <code>Volume</code>.</p> <code>width</code> <code>float</code> <p>Voxel size in mm in the x dimension.</p> <code>height</code> <code>float</code> <p>Voxel size in mm in the y dimension.</p> <code>depth</code> <code>float</code> <p>Voxel size in mm in the z dimension.</p> Source code in <code>element_zstack/volume.py</code> <pre><code>@schema\nclass VoxelSize(dj.Manual):\n\"\"\"Voxel size information about a volume in millimeters.\n\n    Attributes:\n        Volume (foreign key): Primary key from `Volume`.\n        width (float): Voxel size in mm in the x dimension.\n        height (float): Voxel size in mm in the y dimension.\n        depth (float): Voxel size in mm in the z dimension.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; Volume\n    ---\n    width: float # voxel size in mm in the x dimension\n    height: float # voxel size in mm in the y dimension\n    depth: float # voxel size in mm in the z dimension\n    \"\"\"\n</code></pre>"}, {"location": "api/element_zstack/volume/#element_zstack.volume.SegmentationParamSet", "title": "<code>SegmentationParamSet</code>", "text": "<p>         Bases: <code>dj.Lookup</code></p> <p>Parameter set used for segmentation of the volumetric microscopic imaging scan.</p> <p>Attributes:</p> Name Type Description <code>paramset_idx</code> <code>int</code> <p>Unique parameter set identifier.</p> <code>segmentation_method</code> <code>str</code> <p>Name of the segmentation method (e.g.</p> <code>paramset_desc</code> <code>str</code> <p>Optional. Parameter set description.</p> <code>params</code> <code>longblob</code> <p>Parameter set. Dictionary of all applicable</p> <code>paramset_hash</code> <code>uuid</code> <p>A universally unique identifier for the parameter set.</p> Source code in <code>element_zstack/volume.py</code> <pre><code>@schema\nclass SegmentationParamSet(dj.Lookup):\n\"\"\"Parameter set used for segmentation of the volumetric microscopic imaging\n    scan.\n\n    Attributes:\n        paramset_idx (int): Unique parameter set identifier.\n        segmentation_method (str): Name of the segmentation method (e.g.\n        cellpose).\n        paramset_desc (str): Optional. Parameter set description.\n        params (longblob): Parameter set. Dictionary of all applicable\n        parameters for the segmentation method.\n        paramset_hash (uuid): A universally unique identifier for the parameter set.\n    \"\"\"\n\n    definition = \"\"\"\n    paramset_idx: int\n    ---\n    segmentation_method: varchar(32)\n    paramset_desc=\"\": varchar(256)\n    params: longblob\n    paramset_hash: uuid\n    unique index (paramset_hash)\n    \"\"\"\n\n    @classmethod\n    def insert_new_params(\n        cls,\n        segmentation_method: str,\n        params: dict,\n        paramset_desc: str = \"\",\n        paramset_idx: int = None,\n    ):\n\"\"\"Inserts new parameters into the table.\n\n        Args:\n            segmentation_method (str): name of the segmentation method (e.g. cellpose)\n            params (dict): segmentation parameters\n            paramset_desc (str, optional): description of the parameter set\n            paramset_idx (int, optional): Unique parameter set ID. Defaults to None.\n        \"\"\"\n        if paramset_idx is None:\n            paramset_idx = (\n                dj.U().aggr(cls, n=\"max(paramset_idx)\").fetch1(\"n\") or 0\n            ) + 1\n\n        param_dict = {\n            \"segmentation_method\": segmentation_method,\n            \"paramset_desc\": paramset_desc,\n            \"params\": params,\n            \"paramset_idx\": paramset_idx,\n            \"paramset_hash\": dict_to_uuid(\n                {**params, \"segmentation_method\": segmentation_method}\n            ),\n        }\n        param_query = cls &amp; {\"paramset_hash\": param_dict[\"paramset_hash\"]}\n\n        if param_query:  # If the specified param-set already exists\n            existing_paramset_idx = param_query.fetch1(\"paramset_idx\")\n            if (\n                existing_paramset_idx == paramset_idx\n            ):  # If the existing set has the same paramset_idx: job done\n                return\n            else:  # If not same name: human error, trying to add the same paramset with different name\n                raise dj.DataJointError(\n                    f\"The specified param-set already exists\"\n                    f\" - with paramset_idx: {existing_paramset_idx}\"\n                )\n        else:\n            if {\"paramset_idx\": paramset_idx} in cls.proj():\n                raise dj.DataJointError(\n                    f\"The specified paramset_idx {paramset_idx} already exists,\"\n                    f\" please pick a different one.\"\n                )\n            cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_zstack/volume/#element_zstack.volume.SegmentationParamSet.insert_new_params", "title": "<code>insert_new_params(segmentation_method, params, paramset_desc='', paramset_idx=None)</code>  <code>classmethod</code>", "text": "<p>Inserts new parameters into the table.</p> <p>Parameters:</p> Name Type Description Default <code>segmentation_method</code> <code>str</code> <p>name of the segmentation method (e.g. cellpose)</p> required <code>params</code> <code>dict</code> <p>segmentation parameters</p> required <code>paramset_desc</code> <code>str</code> <p>description of the parameter set</p> <code>''</code> <code>paramset_idx</code> <code>int</code> <p>Unique parameter set ID. Defaults to None.</p> <code>None</code> Source code in <code>element_zstack/volume.py</code> <pre><code>@classmethod\ndef insert_new_params(\n    cls,\n    segmentation_method: str,\n    params: dict,\n    paramset_desc: str = \"\",\n    paramset_idx: int = None,\n):\n\"\"\"Inserts new parameters into the table.\n\n    Args:\n        segmentation_method (str): name of the segmentation method (e.g. cellpose)\n        params (dict): segmentation parameters\n        paramset_desc (str, optional): description of the parameter set\n        paramset_idx (int, optional): Unique parameter set ID. Defaults to None.\n    \"\"\"\n    if paramset_idx is None:\n        paramset_idx = (\n            dj.U().aggr(cls, n=\"max(paramset_idx)\").fetch1(\"n\") or 0\n        ) + 1\n\n    param_dict = {\n        \"segmentation_method\": segmentation_method,\n        \"paramset_desc\": paramset_desc,\n        \"params\": params,\n        \"paramset_idx\": paramset_idx,\n        \"paramset_hash\": dict_to_uuid(\n            {**params, \"segmentation_method\": segmentation_method}\n        ),\n    }\n    param_query = cls &amp; {\"paramset_hash\": param_dict[\"paramset_hash\"]}\n\n    if param_query:  # If the specified param-set already exists\n        existing_paramset_idx = param_query.fetch1(\"paramset_idx\")\n        if (\n            existing_paramset_idx == paramset_idx\n        ):  # If the existing set has the same paramset_idx: job done\n            return\n        else:  # If not same name: human error, trying to add the same paramset with different name\n            raise dj.DataJointError(\n                f\"The specified param-set already exists\"\n                f\" - with paramset_idx: {existing_paramset_idx}\"\n            )\n    else:\n        if {\"paramset_idx\": paramset_idx} in cls.proj():\n            raise dj.DataJointError(\n                f\"The specified paramset_idx {paramset_idx} already exists,\"\n                f\" please pick a different one.\"\n            )\n        cls.insert1(param_dict)\n</code></pre>"}, {"location": "api/element_zstack/volume/#element_zstack.volume.SegmentationTask", "title": "<code>SegmentationTask</code>", "text": "<p>         Bases: <code>dj.Manual</code></p> <p>Defines the method and parameter set which will be used to segment a volume in the downstream <code>Segmentation</code> table.  This table currently supports triggering segmentation with <code>cellpose</code>.</p> <p>Attributes:</p> Name Type Description <code>Volume</code> <code>foreign key</code> <p>Primary key from <code>Volume</code>.</p> <code>SegmentationParamSet</code> <code>foreign key</code> <p>Primary key from</p> <code>segmentation_output_dir</code> <code>str</code> <p>Optional. Output directory of the</p> <code>task_mode</code> <code>enum</code> <p><code>Trigger</code> computes segmentation or <code>load</code> imports existing results.</p> Source code in <code>element_zstack/volume.py</code> <pre><code>@schema\nclass SegmentationTask(dj.Manual):\n\"\"\"Defines the method and parameter set which will be used to segment a volume in the downstream `Segmentation` table.  This table currently supports triggering segmentation with `cellpose`.\n\n    Attributes:\n        Volume (foreign key): Primary key from `Volume`.\n        SegmentationParamSet (foreign key): Primary key from\n        `SegmentationParamSet`.\n        segmentation_output_dir (str): Optional. Output directory of the\n        segmented results relative to the root data directory.\n        task_mode (enum): `Trigger` computes segmentation or `load` imports existing results.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; Volume\n    -&gt; SegmentationParamSet\n    ---\n    segmentation_output_dir='': varchar(255)  #  Output directory of the segmented results relative to root data directory\n    task_mode='load': enum('load', 'trigger')\n    \"\"\"\n</code></pre>"}, {"location": "api/element_zstack/volume/#element_zstack.volume.Segmentation", "title": "<code>Segmentation</code>", "text": "<p>         Bases: <code>dj.Computed</code></p> <p>Performs segmentation on the volume (and with the method and parameter set) defined in the <code>SegmentationTask</code> table.</p> <p>Attributes:</p> Name Type Description <code>SegmentationTask</code> <code>foreign key</code> <p>Primary key from <code>SegmentationTask</code>.</p> Source code in <code>element_zstack/volume.py</code> <pre><code>@schema\nclass Segmentation(dj.Computed):\n\"\"\"Performs segmentation on the volume (and with the method and parameter set) defined in the `SegmentationTask` table.\n\n    Attributes:\n        SegmentationTask (foreign key): Primary key from `SegmentationTask`.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; SegmentationTask\n    \"\"\"\n\n    class Mask(dj.Part):\n\"\"\"Details of the masks identified from the segmentation.\n\n        Attributes:\n            Segmentation (foreign key): Primary key from `Segmentation`.\n            mask (int): Unique mask identifier.\n            mask_npix (int): Number of pixels in the mask.\n            mask_center_x (float): Center x coordinate in pixels.\n            mask_center_y (float): Center y coordinate in pixels.\n            mask_center_z (float): Center z coordinate in pixels.\n            mask_xpix (longblob): X coordinates in pixels.\n            mask_ypix (longblob): Y coordinates in pixels.\n            mask_zpix (longblob): Z coordinates in pixels.\n            mask_weights (longblob): Weights of the mask at the indices above.\n        \"\"\"\n\n        definition = \"\"\" # A mask produced by segmentation.\n        -&gt; master\n        mask            : smallint\n        ---\n        mask_npix       : int       # number of pixels in ROIs\n        mask_center_x   : float     # X component of the 3D mask centroid in pixel units\n        mask_center_y   : float     # Y component of the 3D mask centroid in pixel units\n        mask_center_z   : float     # Z component of the 3D mask centroid in pixel units\n        mask_xpix       : longblob  # x coordinates in pixels units\n        mask_ypix       : longblob  # y coordinates in pixels units\n        mask_zpix       : longblob  # z coordinates in pixels units\n        mask_weights    : longblob  # weights of the mask at the indices above\n        \"\"\"\n\n    def make(self, key):\n\"\"\"Populate the Segmentation and Segmentation.Mask tables with results of cellpose segmentation.\"\"\"\n\n        task_mode, seg_method, output_dir, params = (\n            SegmentationTask * SegmentationParamSet &amp; key\n        ).fetch1(\n            \"task_mode\", \"segmentation_method\", \"segmentation_output_dir\", \"params\"\n        )\n        output_dir = find_full_path(get_volume_root_data_dir(), output_dir).as_posix()\n        if task_mode == \"trigger\" and seg_method.lower() == \"cellpose\":\n            from cellpose import models as cellpose_models\n\n            volume_relative_path = (Volume &amp; key).fetch1(\"volume_file_path\")\n            volume_file_path = find_full_path(\n                get_volume_root_data_dir(), volume_relative_path\n            ).as_posix()\n            volume_data = TiffFile(volume_file_path).asarray()\n\n            model = cellpose_models.CellposeModel(model_type=params[\"model_type\"])\n            cellpose_results = model.eval(\n                [volume_data],\n                diameter=params[\"diameter\"],\n                channels=params.get(\"channels\", [[0, 0]]),\n                min_size=params[\"min_size\"],\n                z_axis=0,\n                do_3D=params[\"do_3d\"],\n                anisotropy=params[\"anisotropy\"],\n                progress=True,\n            )\n            masks, flows, styles = cellpose_results\n\n            mask_entries = []\n            for mask_id in set(masks[0].flatten()) - {0}:\n                mask = np.argwhere(masks[0] == mask_id)\n                mask_zpix, mask_ypix, mask_xpix = mask.T\n                mask_npix = mask.shape[0]\n                mask_center_z, mask_center_y, mask_center_x = mask.mean(axis=0)\n                mask_weights = np.full_like(mask_zpix, 1)\n                mask_entries.append(\n                    {\n                        **key,\n                        \"mask\": mask_id,\n                        \"mask_npix\": mask_npix,\n                        \"mask_center_x\": mask_center_x,\n                        \"mask_center_y\": mask_center_y,\n                        \"mask_center_z\": mask_center_z,\n                        \"mask_xpix\": mask_xpix,\n                        \"mask_ypix\": mask_ypix,\n                        \"mask_zpix\": mask_zpix,\n                        \"mask_weights\": mask_weights,\n                    }\n                )\n        else:\n            raise NotImplementedError\n\n        self.insert1(key)\n        self.Mask.insert(mask_entries)\n</code></pre>"}, {"location": "api/element_zstack/volume/#element_zstack.volume.Segmentation.Mask", "title": "<code>Mask</code>", "text": "<p>         Bases: <code>dj.Part</code></p> <p>Details of the masks identified from the segmentation.</p> <p>Attributes:</p> Name Type Description <code>Segmentation</code> <code>foreign key</code> <p>Primary key from <code>Segmentation</code>.</p> <code>mask</code> <code>int</code> <p>Unique mask identifier.</p> <code>mask_npix</code> <code>int</code> <p>Number of pixels in the mask.</p> <code>mask_center_x</code> <code>float</code> <p>Center x coordinate in pixels.</p> <code>mask_center_y</code> <code>float</code> <p>Center y coordinate in pixels.</p> <code>mask_center_z</code> <code>float</code> <p>Center z coordinate in pixels.</p> <code>mask_xpix</code> <code>longblob</code> <p>X coordinates in pixels.</p> <code>mask_ypix</code> <code>longblob</code> <p>Y coordinates in pixels.</p> <code>mask_zpix</code> <code>longblob</code> <p>Z coordinates in pixels.</p> <code>mask_weights</code> <code>longblob</code> <p>Weights of the mask at the indices above.</p> Source code in <code>element_zstack/volume.py</code> <pre><code>class Mask(dj.Part):\n\"\"\"Details of the masks identified from the segmentation.\n\n    Attributes:\n        Segmentation (foreign key): Primary key from `Segmentation`.\n        mask (int): Unique mask identifier.\n        mask_npix (int): Number of pixels in the mask.\n        mask_center_x (float): Center x coordinate in pixels.\n        mask_center_y (float): Center y coordinate in pixels.\n        mask_center_z (float): Center z coordinate in pixels.\n        mask_xpix (longblob): X coordinates in pixels.\n        mask_ypix (longblob): Y coordinates in pixels.\n        mask_zpix (longblob): Z coordinates in pixels.\n        mask_weights (longblob): Weights of the mask at the indices above.\n    \"\"\"\n\n    definition = \"\"\" # A mask produced by segmentation.\n    -&gt; master\n    mask            : smallint\n    ---\n    mask_npix       : int       # number of pixels in ROIs\n    mask_center_x   : float     # X component of the 3D mask centroid in pixel units\n    mask_center_y   : float     # Y component of the 3D mask centroid in pixel units\n    mask_center_z   : float     # Z component of the 3D mask centroid in pixel units\n    mask_xpix       : longblob  # x coordinates in pixels units\n    mask_ypix       : longblob  # y coordinates in pixels units\n    mask_zpix       : longblob  # z coordinates in pixels units\n    mask_weights    : longblob  # weights of the mask at the indices above\n    \"\"\"\n</code></pre>"}, {"location": "api/element_zstack/volume/#element_zstack.volume.Segmentation.make", "title": "<code>make(key)</code>", "text": "<p>Populate the Segmentation and Segmentation.Mask tables with results of cellpose segmentation.</p> Source code in <code>element_zstack/volume.py</code> <pre><code>def make(self, key):\n\"\"\"Populate the Segmentation and Segmentation.Mask tables with results of cellpose segmentation.\"\"\"\n\n    task_mode, seg_method, output_dir, params = (\n        SegmentationTask * SegmentationParamSet &amp; key\n    ).fetch1(\n        \"task_mode\", \"segmentation_method\", \"segmentation_output_dir\", \"params\"\n    )\n    output_dir = find_full_path(get_volume_root_data_dir(), output_dir).as_posix()\n    if task_mode == \"trigger\" and seg_method.lower() == \"cellpose\":\n        from cellpose import models as cellpose_models\n\n        volume_relative_path = (Volume &amp; key).fetch1(\"volume_file_path\")\n        volume_file_path = find_full_path(\n            get_volume_root_data_dir(), volume_relative_path\n        ).as_posix()\n        volume_data = TiffFile(volume_file_path).asarray()\n\n        model = cellpose_models.CellposeModel(model_type=params[\"model_type\"])\n        cellpose_results = model.eval(\n            [volume_data],\n            diameter=params[\"diameter\"],\n            channels=params.get(\"channels\", [[0, 0]]),\n            min_size=params[\"min_size\"],\n            z_axis=0,\n            do_3D=params[\"do_3d\"],\n            anisotropy=params[\"anisotropy\"],\n            progress=True,\n        )\n        masks, flows, styles = cellpose_results\n\n        mask_entries = []\n        for mask_id in set(masks[0].flatten()) - {0}:\n            mask = np.argwhere(masks[0] == mask_id)\n            mask_zpix, mask_ypix, mask_xpix = mask.T\n            mask_npix = mask.shape[0]\n            mask_center_z, mask_center_y, mask_center_x = mask.mean(axis=0)\n            mask_weights = np.full_like(mask_zpix, 1)\n            mask_entries.append(\n                {\n                    **key,\n                    \"mask\": mask_id,\n                    \"mask_npix\": mask_npix,\n                    \"mask_center_x\": mask_center_x,\n                    \"mask_center_y\": mask_center_y,\n                    \"mask_center_z\": mask_center_z,\n                    \"mask_xpix\": mask_xpix,\n                    \"mask_ypix\": mask_ypix,\n                    \"mask_zpix\": mask_zpix,\n                    \"mask_weights\": mask_weights,\n                }\n            )\n    else:\n        raise NotImplementedError\n\n    self.insert1(key)\n    self.Mask.insert(mask_entries)\n</code></pre>"}, {"location": "api/element_zstack/export/bossdb_interface/", "title": "bossdb_interface.py", "text": ""}, {"location": "api/element_zstack/export/bossdb_interface/#element_zstack.export.bossdb_interface.BossDBUpload", "title": "<code>BossDBUpload</code>", "text": "<p>Upload data to bossdb from a DataJoint pipeline.</p> Source code in <code>element_zstack/export/bossdb_interface.py</code> <pre><code>class BossDBUpload:\n\"\"\"Upload data to bossdb from a DataJoint pipeline.\"\"\"\n\n    def __init__(\n        self,\n        url: str,\n        volume_data: np.ndarray,  # Numpy array of the volumetric data\n        data_description: str,\n        voxel_size: Tuple[int, int, int],  # voxel size in ZYX order\n        voxel_units: str,  # The size units of a voxel\n        upload_increment: Optional[int] = 16,  # How many z slices to upload at once\n        retry_max: Optional[int] = 3,  # Number of retries to upload a single increment\n        overwrite: Optional[bool] = False,  # Overwrite existing data\n    ):\n\"\"\"Required information for data upload to bossdb.\n\n        Args:\n            url (str): Bossdb URL where data will be uploaded.\n            volume_data (np.ndarray): Numpy array of the volumetric data\n            data_description (str): either `image` or `annotation`.\n            voxel_size (Tuple[int, int, int]): Voxel size of the image in z,y,x\n            format.\n            voxel_units (str): Voxel units as string.\n            upload_increment (int, optional): Number of z slices to upload at\n            once.\n            retry_max (int, optional): Number of retries to upload a single\n            increment.\n            overwrite (bool, optional): Overwrite existing data.\n        \"\"\"\n\n        self._url = url\n        self.url_bits = _parse_bossdb_uri(url)\n        self._volume_data = volume_data\n        self._voxel_size = tuple(float(i) for i in voxel_size)\n        self._voxel_units = voxel_units\n        self._data_description = data_description\n        self._upload_increment = upload_increment\n        self._retry_max = retry_max\n        self._overwrite = overwrite\n        self.description = \"Uploaded via DataJoint\"\n        self._resources = dict()\n        self._shape_zyx = tuple(int(i) for i in self._volume_data.shape)\n\n        try:\n            type(array(self._url))\n        except HTTPError:\n            self.url_exists = False\n        else:\n            self.url_exists = True\n        if not overwrite and self.url_exists:\n            logger.warning(\n                f\"Dataset already exists at {self._url}\\n\"\n                + \" To overwrite, set `overwrite` to True\"\n            )\n            return\n\n        if not self.url_exists:\n            self.try_create_new()\n\n    def upload(self):\n\"\"\"Upload data to bossdb.\"\"\"\n        z_max = self._shape_zyx[0]\n        boss_dataset = array(\n            self._url,\n            extents=self._shape_zyx,\n            dtype=str(self._volume_data.dtype),\n            voxel_size=self._voxel_size,\n            voxel_unit=self._voxel_units,\n        )\n        for i in tqdm(range(0, z_max, self._upload_increment)):\n            if i + self._upload_increment &gt; self._shape_zyx[0]:\n                # We're at the end of the stack, so upload the remaining images.\n                images = self._volume_data[i : self._shape_zyx[0], :, :]\n                retry_count = 0\n                while True:\n                    try:\n                        boss_dataset[\n                            i : i + images.shape[0],\n                            0 : images.shape[1],\n                            0 : images.shape[2],\n                        ] = images\n                        break\n                    except Exception as e:\n                        print(f\"Error uploading chunk {i}-{i + images.shape[0]}: {e}\")\n                        retry_count += 1\n                        if retry_count &gt; self._retry_max:\n                            raise e\n                        print(\"Retrying...\")\n                        continue\n            else:\n                images = self._volume_data[i : i + self._upload_increment]\n                retry_count = 0\n                while True:\n                    try:\n                        boss_dataset[\n                            i : i + images.shape[0],\n                            0 : images.shape[1],\n                            0 : images.shape[2],\n                        ] = images\n                        break\n                    except Exception as e:\n                        print(f\"Error uploading chunk {i}-{i + images.shape[0]}: {e}\")\n                        retry_count += 1\n                        if retry_count &gt; self._retry_max:\n                            raise e\n                        print(\"Retrying...\")\n                        continue\n\n    @property\n    def resources(self):\n\"\"\"Create the resources objects required for uploading data to bossdb.\"\"\"\n        # Default resources for creating channels\n        coord_name = f\"CF_{self.url_bits.collection}_{self.url_bits.experiment}\"\n        if not self._resources:\n            self._resources = dict(\n                collection=CollectionResource(\n                    name=self.url_bits.collection, description=self.description\n                ),\n                coord_frame=CoordinateFrameResource(\n                    name=coord_name,\n                    description=self.description,\n                    x_start=0,\n                    x_stop=self._shape_zyx[2],\n                    y_start=0,\n                    y_stop=self._shape_zyx[1],\n                    z_start=0,\n                    z_stop=self._shape_zyx[0],\n                    x_voxel_size=self._voxel_size[2],\n                    y_voxel_size=self._voxel_size[1],\n                    z_voxel_size=self._voxel_size[0],\n                ),\n                experiment=ExperimentResource(\n                    name=self.url_bits.experiment,\n                    collection_name=self.url_bits.collection,\n                    coord_frame=coord_name,\n                    description=self.description,\n                ),\n                channel_resource=ChannelResource(\n                    name=self.url_bits.channel,\n                    collection_name=self.url_bits.collection,\n                    experiment_name=self.url_bits.experiment,\n                    type=self._data_description,\n                    description=self.description,\n                    datatype=str(self._volume_data.dtype),\n                ),\n                channel=ChannelResource(\n                    name=self.url_bits.channel,\n                    collection_name=self.url_bits.collection,\n                    experiment_name=self.url_bits.experiment,\n                    type=self._data_description,\n                    description=self.description,\n                    datatype=str(self._volume_data.dtype),\n                    sources=[],\n                ),\n            )\n        return self._resources\n\n    def try_create_new(self):\n\"\"\"Create new resources on bossdb.\"\"\"\n        remote = BossRemote()\n\n        # Make collection\n        _ = self._get_or_create(remote=remote, obj=self.resources[\"collection\"])\n\n        # Make coord frame\n        true_coord_frame = self._get_or_create(\n            remote=remote, obj=self.resources[\"coord_frame\"]\n        )\n\n        # Set Experiment based on coord frame\n        experiment = self.resources[\"experiment\"]\n        experiment.coord_frame = true_coord_frame.name\n        _ = self._get_or_create(remote=remote, obj=experiment)\n\n        # Set channel based on resource\n        channel_resource = self._get_or_create(\n            remote=remote, obj=self.resources[\"channel_resource\"]\n        )\n        channel = self.resources[\"channel\"]\n        channel.sources = [channel_resource.name]\n        _ = self._get_or_create(remote=remote, obj=channel)\n\n    def _get_or_create(self, remote, obj):\n\"\"\"Check if a resource exists on bossdb.\"\"\"\n        try:\n            result = remote.get_project(obj)\n        except HTTPError:\n            logger.info(f\"Creating {obj.name}\")\n            result = remote.create_project(obj)\n        return result\n</code></pre>"}, {"location": "api/element_zstack/export/bossdb_interface/#element_zstack.export.bossdb_interface.BossDBUpload.__init__", "title": "<code>__init__(url, volume_data, data_description, voxel_size, voxel_units, upload_increment=16, retry_max=3, overwrite=False)</code>", "text": "<p>Required information for data upload to bossdb.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Bossdb URL where data will be uploaded.</p> required <code>volume_data</code> <code>np.ndarray</code> <p>Numpy array of the volumetric data</p> required <code>data_description</code> <code>str</code> <p>either <code>image</code> or <code>annotation</code>.</p> required <code>voxel_size</code> <code>Tuple[int, int, int]</code> <p>Voxel size of the image in z,y,x</p> required <code>voxel_units</code> <code>str</code> <p>Voxel units as string.</p> required <code>upload_increment</code> <code>int</code> <p>Number of z slices to upload at</p> <code>16</code> <code>retry_max</code> <code>int</code> <p>Number of retries to upload a single</p> <code>3</code> <code>overwrite</code> <code>bool</code> <p>Overwrite existing data.</p> <code>False</code> Source code in <code>element_zstack/export/bossdb_interface.py</code> <pre><code>def __init__(\n    self,\n    url: str,\n    volume_data: np.ndarray,  # Numpy array of the volumetric data\n    data_description: str,\n    voxel_size: Tuple[int, int, int],  # voxel size in ZYX order\n    voxel_units: str,  # The size units of a voxel\n    upload_increment: Optional[int] = 16,  # How many z slices to upload at once\n    retry_max: Optional[int] = 3,  # Number of retries to upload a single increment\n    overwrite: Optional[bool] = False,  # Overwrite existing data\n):\n\"\"\"Required information for data upload to bossdb.\n\n    Args:\n        url (str): Bossdb URL where data will be uploaded.\n        volume_data (np.ndarray): Numpy array of the volumetric data\n        data_description (str): either `image` or `annotation`.\n        voxel_size (Tuple[int, int, int]): Voxel size of the image in z,y,x\n        format.\n        voxel_units (str): Voxel units as string.\n        upload_increment (int, optional): Number of z slices to upload at\n        once.\n        retry_max (int, optional): Number of retries to upload a single\n        increment.\n        overwrite (bool, optional): Overwrite existing data.\n    \"\"\"\n\n    self._url = url\n    self.url_bits = _parse_bossdb_uri(url)\n    self._volume_data = volume_data\n    self._voxel_size = tuple(float(i) for i in voxel_size)\n    self._voxel_units = voxel_units\n    self._data_description = data_description\n    self._upload_increment = upload_increment\n    self._retry_max = retry_max\n    self._overwrite = overwrite\n    self.description = \"Uploaded via DataJoint\"\n    self._resources = dict()\n    self._shape_zyx = tuple(int(i) for i in self._volume_data.shape)\n\n    try:\n        type(array(self._url))\n    except HTTPError:\n        self.url_exists = False\n    else:\n        self.url_exists = True\n    if not overwrite and self.url_exists:\n        logger.warning(\n            f\"Dataset already exists at {self._url}\\n\"\n            + \" To overwrite, set `overwrite` to True\"\n        )\n        return\n\n    if not self.url_exists:\n        self.try_create_new()\n</code></pre>"}, {"location": "api/element_zstack/export/bossdb_interface/#element_zstack.export.bossdb_interface.BossDBUpload.upload", "title": "<code>upload()</code>", "text": "<p>Upload data to bossdb.</p> Source code in <code>element_zstack/export/bossdb_interface.py</code> <pre><code>def upload(self):\n\"\"\"Upload data to bossdb.\"\"\"\n    z_max = self._shape_zyx[0]\n    boss_dataset = array(\n        self._url,\n        extents=self._shape_zyx,\n        dtype=str(self._volume_data.dtype),\n        voxel_size=self._voxel_size,\n        voxel_unit=self._voxel_units,\n    )\n    for i in tqdm(range(0, z_max, self._upload_increment)):\n        if i + self._upload_increment &gt; self._shape_zyx[0]:\n            # We're at the end of the stack, so upload the remaining images.\n            images = self._volume_data[i : self._shape_zyx[0], :, :]\n            retry_count = 0\n            while True:\n                try:\n                    boss_dataset[\n                        i : i + images.shape[0],\n                        0 : images.shape[1],\n                        0 : images.shape[2],\n                    ] = images\n                    break\n                except Exception as e:\n                    print(f\"Error uploading chunk {i}-{i + images.shape[0]}: {e}\")\n                    retry_count += 1\n                    if retry_count &gt; self._retry_max:\n                        raise e\n                    print(\"Retrying...\")\n                    continue\n        else:\n            images = self._volume_data[i : i + self._upload_increment]\n            retry_count = 0\n            while True:\n                try:\n                    boss_dataset[\n                        i : i + images.shape[0],\n                        0 : images.shape[1],\n                        0 : images.shape[2],\n                    ] = images\n                    break\n                except Exception as e:\n                    print(f\"Error uploading chunk {i}-{i + images.shape[0]}: {e}\")\n                    retry_count += 1\n                    if retry_count &gt; self._retry_max:\n                        raise e\n                    print(\"Retrying...\")\n                    continue\n</code></pre>"}, {"location": "api/element_zstack/export/bossdb_interface/#element_zstack.export.bossdb_interface.BossDBUpload.resources", "title": "<code>resources</code>  <code>property</code>", "text": "<p>Create the resources objects required for uploading data to bossdb.</p>"}, {"location": "api/element_zstack/export/bossdb_interface/#element_zstack.export.bossdb_interface.BossDBUpload.try_create_new", "title": "<code>try_create_new()</code>", "text": "<p>Create new resources on bossdb.</p> Source code in <code>element_zstack/export/bossdb_interface.py</code> <pre><code>def try_create_new(self):\n\"\"\"Create new resources on bossdb.\"\"\"\n    remote = BossRemote()\n\n    # Make collection\n    _ = self._get_or_create(remote=remote, obj=self.resources[\"collection\"])\n\n    # Make coord frame\n    true_coord_frame = self._get_or_create(\n        remote=remote, obj=self.resources[\"coord_frame\"]\n    )\n\n    # Set Experiment based on coord frame\n    experiment = self.resources[\"experiment\"]\n    experiment.coord_frame = true_coord_frame.name\n    _ = self._get_or_create(remote=remote, obj=experiment)\n\n    # Set channel based on resource\n    channel_resource = self._get_or_create(\n        remote=remote, obj=self.resources[\"channel_resource\"]\n    )\n    channel = self.resources[\"channel\"]\n    channel.sources = [channel_resource.name]\n    _ = self._get_or_create(remote=remote, obj=channel)\n</code></pre>"}, {"location": "tutorials/", "title": "Tutorials", "text": "<ul> <li>DataJoint Elements are modular pipelines that can be connected into a complete workflow.  Workflow Zstack is an example that combines five DataJoint Elements - Lab, Animal, Session, Calcium Imaging, and ZStack.</li> </ul> <ul> <li>Workflow ZStack includes an interactive tutorial on GitHub Codespaces, which is configured for users to run the pipeline.</li> </ul> <ul> <li>In the interactive tutorial, the example notebook describes the pipeline and provides instructions for running the pipeline.</li> </ul>"}, {"location": "tutorials/#installation-instructions-for-active-projects", "title": "Installation Instructions for Active Projects", "text": "<ul> <li>The Workflow ZStack described above can be modified for a user's specific experimental requirements and thereby used in active projects.</li> </ul> <ul> <li>The GitHub Codespace and Dev Container is configured for tutorials and prototyping. We recommend users to configure a database specifically for production pipelines.  Instructions for a local installation of the integrated development environment with a database can be found on the User Guide page.</li> </ul>"}, {"location": "tutorials/#example-export-to-bossdb", "title": "Example Export to BossDB", "text": ""}, {"location": "tutorials/#example-visualization-on-neuroglancer", "title": "Example Visualization on Neuroglancer", "text": ""}]}